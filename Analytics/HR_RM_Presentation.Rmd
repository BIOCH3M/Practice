<style>
body {
    overflow: scroll;
}
</style>
---
title: "HR Presentation"
output: ioslides_presentation

---

### By: ALEXANDR SURIN

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)

#library
library(flexdashboard)
library(tidyverse)
library(e1071)
library(MASS)
library(tree)
library(randomForest)
library(gbm)
library(dplyr)
library(nnet)
library(highcharter)
library(plotly)
```

## Project Overview


![Why are our best and most experienced employees leaving prematurely?](https://www.kaggle.com/static/images/site-logo.png)

* Analysis of 'Human Resources' Data from kaggle
    + Data Exploration
    + Machine Learning Approach
    + Findings and Discussion
  


## Data Exploration: Given 
<div class="columns-2">

* Independent Variables:
    + Satisfaction Level
    + Last evaluation
    + Number of projects
    + Average monthly hours
    + Time spent at the company
    + Whether they have had a work accident
    + Whether they have had a promotion in the last 5 years
    + Departments (column sales)
    + Salary
    
* Dependent Variable:
    + Whether the employee has left



### Number of Records

```{r}
data<-read.csv("/Users/asurin/Documents/GITHUB_PUBLIC/HR_comma_sep.csv")
numrow <- nrow(data)
valueBox(numrow, icon = "fa-pencil")
```

### Number of Variables

```{r,echo=FALSE}
data<-read.csv("/Users/asurin/Documents/GITHUB_PUBLIC/HR_comma_sep.csv")
numcol<- ncol(data)
valueBox(numcol, icon = "fa-pencil")

```

</div>


## Data Exploration: Data Summary

Quick Look and Summary of HR data

```{r, echo=FALSE}
data<-read.csv("/Users/asurin/Documents/GITHUB_PUBLIC/HR_comma_sep.csv")
head(data)
summary(data)
```

We see that variables Sales and Salary are categorical, so they will need to be converted
to factors.


## Data Exploration: Categorical Variables
<div class="columns-2">
### Salary

```{r}
data<-read.csv("/Users/asurin/Documents/GITHUB_PUBLIC/HR_comma_sep.csv")
pie.sal<-as.character(data$salary)

hchart(pie.sal, type = "pie")

```

### Sales

```{r}
data<-read.csv("/Users/asurin/Documents/GITHUB_PUBLIC/HR_comma_sep.csv")
pie.sale<-as.character(data$sales)

hchart(pie.sale, type = "pie")
```

</div>

## Data Exploration: Insight

```{r}
data<-read.csv("/Users/asurin/Documents/GITHUB_PUBLIC/HR_comma_sep.csv") %>% mutate(left = ifelse(left==1, "Left", "Stayed"))
hcboxplot(x = data$satisfaction_level,var= data$salary, var2 = data$left,
          outliers = FALSE) %>% 
  hc_chart(type = "column") %>% # to put box vertical
  hc_title(text = "Who Left: Satisfaction Level vs Salary") %>% 
  hc_yAxis(title = list(text = "Satisfaction Level")) %>% 
  hc_xAxis(title=list(text = "Salary"))

hcboxplot(x = data$satisfaction_level,var= data$sales, var2 = data$left,
          outliers = FALSE) %>% 
  hc_chart(type = "column") %>% # to put box vertical
  hc_title(text = "Who Left: Satisfaction Level vs Sales") %>% 
  hc_yAxis(title = list(text = "Satisfaction Level")) %>% 
  hc_xAxis(title=list(text = "Sales"))

hcboxplot(x = data$last_evaluation,var= data$sales, var2 = data$left,
          outliers = FALSE) %>% 
  hc_chart(type = "column") %>% # to put box vertical
  hc_title(text = "Who Left: Last Evaluation vs Sales") %>% 
  hc_yAxis(title = list(text = "Last Evaluation")) %>% 
  hc_xAxis(title=list(text = "Sales"))

hcboxplot(x = data$time_spend_company,var= data$sales, var2 = data$left,
          outliers = FALSE) %>% 
  hc_chart(type = "column") %>% # to put box vertical
  hc_title(text = "Who Left: Time with Company vs Sales") %>% 
  hc_yAxis(title = list(text = "Time with Company")) %>% 
  hc_xAxis(title=list(text = "Sales"))

hcboxplot(x = data$number_project, var= data$sales, var2 = data$left,
          outliers = FALSE) %>% 
  hc_chart(type = "column") %>% # to put box vertical
  hc_title(text = "Who Left: Number of Projects vs Sales") %>% 
  hc_yAxis(title = list(text = "Number of Projects")) %>% 
  hc_xAxis(title=list(text = "Sales"))


```

##Data Exploration: Observation Summary

* Satisfaction Level vs Salary
    + People who left the company had a median of 0.41 satisfaction level across all salaries compared to a median of 0.69 satisfaction level for those who stayed. However, it is worth noting that people who had high salaries have very similar satisfaction level compared to much broader ranges for those with low and medium salaries.
  
* Satisfaction Level vs Sales
    + The median satisfaction level for those people who have left the company howers around 0.40 whereas those who stayed at 0.67 across all departments. Most unsatisfied individuals who left were part of the accounting department (negative skewed), whereas the most satisfied were in marketing and product management departments (positive skewed). 
    
* Last Evaluation vs Sales
    + Immediately it s improtant to recognize that people who left  recieved higher evaluation scores than those people who stayed for all departments with the exception of Accounting, HR and Marketing.
    
* Time with Company vs Sales
    + Individuals who left spent around 4 years with the exception of the HR department(3 years). Also, we see that people who stayed with the company have a median of 3 years at the company. 
    
* Number of Projects vs Sales
    + We see that the medium number of projects for those that stayed howevers around 4 for all departments with the majority doing at least 3 projects. Individuals who left had greater differences in project invovement, however, the median still howered at 4 with the exception of HR and Marketing departments.
    
## Machine Learning: Approach

* This is a classification problem because our response variable 'left' is binary:left (1) or stayed (0).
    + We can not use linear regression because some of our estimates will be outside the [0,1] interval making them hard to interpret as probabilities.

* General Idea:
    + Split HR dataset into Train (80%) and Test (20%).
    + Run different classifers on the Train dataset and evaluate classifer performance based on Test Error Rates.
    + Use the best classifer to predict.

* Classifiers used:
    + Logistic Regression
    + Linear Discirminant Analysis (LDA)
    + Quadratic Disciminanat Analysis (QDA)
    + Classification Tree
    + Bagging Tree
    + Boosting Tree
    + Neuaral Networks

* What's the catch:Bias-Variance Trade-off
    + More complex models are prone to overfitting: Low Bias but High Variance.

![](https://i.stack.imgur.com/r7QFy.png)

```{r, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)

data<-read.csv("/Users/asurin/Documents/GITHUB_PUBLIC/HR_comma_sep.csv")

refcols <- c("left")
#bringing respnonse variable 'left' to the end of the dataframe
data <- data[, c(setdiff(names(data), refcols), refcols )]

#convert data type
data$salary<-as.numeric(factor(data$salary))
data$sales<-as.numeric(factor(data$sales))
data<-as.data.frame(apply(data, 2, function(x){
  as.numeric(x)
}))
#Check for NA values
data <- na.omit(data)
#create test and training sets
set.seed(1)
train <- sample(1:nrow(data),.8*nrow(data))
training <- data[train,]
#select all data which is not train for test
test <- data[-train,]
#test records 'left' column
actual.left<-test$left
```
## Machine Learning: Logistic Regression
```{r, echoe=TRUE}
#Log---using binomial since it is a Yes or No category
glm.fit<-glm(left~., family=binomial, data=training)
summary(glm.fit)

glm.prob<-predict(glm.fit, test,type = "response")
#####Confusion Matrix
#assign "No"(0) to all test records
glm.pred=rep(0,3000)
#now substitue "Yes"(1) for all varialbes which were over .5
glm.pred[glm.prob>.5]= 1
```
- Confusion Matrix
```{r, echo=TRUE}
#finally create confusion matrix
table(glm.pred,actual.left)
```
- Test Error Rate
```{r, echo=TRUE}
#calculate test error
glm.fit.err<-mean(glm.pred!=actual.left)
glm.fit.err
```
## Machine Learning: Linear Discirminant Analysis (LDA)
```{r, echo=TRUE}
lda.fit<-lda(left~., data=training)
lda.pred<-predict(lda.fit, test)
lda.class<-lda.pred$class

```
- Confusion Matrix
```{r, echo=TRUE}
table(lda.class, actual.left)

```
- Test Error Rate
```{r, echo=TRUE}
lda.fit.err<-mean(lda.class !=actual.left)
lda.fit.err

```
## Machine Learning: Quadratic Disciminanat Analysis (QDA)
```{r, echo=TRUE}
qda.fit<-qda(left~., data=training)
qda.pred<-predict(qda.fit, test)
qda.class<-qda.pred$class

```
- Confusion Matrix
```{r, echo=TRUE}
table(qda.class, actual.left)

```
- Test Error Rate
```{r, echo=TRUE}
qda.fit.err<-mean(qda.class !=actual.left)
qda.fit.err

```
## Machine Learning: Classification Tree
```{r, echo=TRUE}
tree.fit<-tree(as.factor(left)~., data=training)
summary(tree.fit)
plot(tree.fit)
text(tree.fit,pretty = 0)
```

```{r, echo=TRUE}
tree.pred<-predict(tree.fit,test, type="class")
table(tree.pred, actual.left)

```
- Test Error Rate
```{r, echo=TRUE}
tree.fit.err<-mean(tree.pred !=actual.left)
tree.fit.err
```
- Run Cross Validation to determine Tree size
- Here we see that using 10 notes provides us with best tree. We don't prune the tree.
```{r, echo=TRUE}
cv.tree.fit<-cv.tree(tree.fit)
par(mfrow=c(1,2))
#you can choose the tree with more pruned see if it helps by this graph
plot(cv.tree.fit$size,cv.tree.fit$dev, xlim = c(0,12),xlab="Size", ylab = "CV Dev")
```




## Machine Learning: Bagging Tree
```{r, echo=TRUE}
set.seed(1)
bag.tree.fit<-randomForest(as.factor(left)~.,data =training, mtry=9,importance=TRUE)
bag.tree.fit

```
- Confusion Matrix
```{r, echo=TRUE}
bag.tree.pred<-predict(bag.tree.fit,test, type="class")
table(bag.tree.pred, actual.left)

```
- Test Error Rate
```{r, echo=TRUE}
bag.tree.fit.err<-mean(bag.tree.pred !=actual.left)
bag.tree.fit.err
```
## Machine Learning: Boosting Tree
```{r, echo=TRUE}
boost.tree.fit<-gbm(left~., data=training, distribution="bernoulli", n.trees = 500, interaction.depth=2)
summary(boost.tree.fit)


```
- Confusion Matrix
```{r, echo=TRUE}
boost.tree.pred<-round(predict(boost.tree.fit,test, type="response", n.trees=500))
table(boost.tree.pred, actual.left)

```
- Test Error Rate
```{r, echo=TRUE}
boost.tree.fit.err<-1-((2260)/(740+2260))
boost.tree.fit.err

```
## Machine Learning: Neuaral Networks
```{r, echo=TRUE}
ns <- nnet(left ~ .,data=training,size=5,linout=FALSE,decay=5e-4)
```

```{r, echo=TRUE}
pr.ns <- predict(ns,test[,1:9])
pr.nn_2 <- pr.ns*sd(data$left) + mean(data$left)
mse.ns <- sum((test$left - pr.nn_2)^2)/nrow(pr.nn_2)
mse.ns

```
```{r, echo=FALSE}
models<-c("LOG REG","LDA","QDA","CLASS. TREE", "BAGGING",
          "BOOSTING","NEURAL NETWORKS")

ter.result<-c(glm.fit.err,lda.fit.err,qda.fit.err,tree.fit.err,bag.tree.fit.err,boost.tree.fit.err, mse.ns)
v3<- 1:7
ter.table<-data.frame(Classifier= models,TER_ALL = ter.result) %>% arrange(TER_ALL)


```

## Machine Learning: Prediction Using 'Good People' Only

* I now repeat everything from above using the following restrictions on data:
    + Last Evaluation >= 0.80
    + Time with Company >= 3
    + Number of Projects >= 3
```{r, echo=FALSE, include = FALSE}
#FINAL PREDICTION USING BAGGED TREES model and run on data which includes only "Good People"

data.2<-read.csv("/Users/asurin/Documents/GITHUB_PUBLIC/HR_comma_sep.csv") %>% filter (last_evaluation >= 0.80 & time_spend_company >= 3 & number_project >= 3)

refcols <- c("left")
#bringing respnonse variable 'left' to the end of the data.2frame
data.2 <- data.2[, c(setdiff(names(data.2), refcols), refcols )]
data.2$salary<-as.numeric(factor(data.2$salary))
data.2$sales<-as.numeric(factor(data.2$sales))  
data.2<-as.data.frame(apply(data.2, 2, function(x){
  as.numeric(x)
}))
set.seed(1)
train <- sample(1:nrow(data.2),.8*nrow(data.2))
training.2 <- data.2[train,]
#select all data.2 which is not train for test.2
test.2 <- data.2[-train,]
#test.2 records 'left' column
actual.left.2<-test.2$left

##########################LOG

#Log---using binomial since it is a Yes or No category
glm.fit.good<-glm(left~., family=binomial, data =training.2)
# summary(glm.fit.good)

glm.prob.good<-predict(glm.fit.good, test.2,type = "response")
#####Confusion Matrix
#assign "No"(0) to all test.2 records
glm.pred.good=rep(0,889)
#now substitue "Yes"(1) for all varialbes which were over .5
glm.pred.good[glm.prob.good>.5]= 1
#finally create confusion matrix
c1<-table(glm.pred.good,actual.left.2)
#calculate test.2 error
glm.fit.err.good<-mean(glm.pred.good!=actual.left.2)

##########################LDA

lda.fit.good<-lda(left~., data=training.2)
lda.pred.good<-predict(lda.fit.good, test.2)
lda.class.good<-lda.pred.good$class
c2<-table(lda.class.good, actual.left.2)
#Error
lda.fit.err.good<-mean(lda.class.good !=actual.left.2)
# lda.fit.err.good

##########################QDA

qda.fit.good<-qda(left~., data=training.2)
qda.pred.good<-predict(qda.fit.good, test.2)
qda.class.good<-qda.pred.good$class
c3<-table(qda.class.good, actual.left.2)
#Error
qda.fit.err.good<-mean(qda.class.good !=actual.left.2)
# qda.fit.err.good


########################## Classification TREES
tree.fit.good<-tree(as.factor(left)~., data=training.2)
# summary(tree.fit.good)
# plot(tree.fit.good)
# text(tree.fit.good,pretty = 0)
#look at the ERROR
tree.pred.good<-predict(tree.fit.good,test.2, type="class")
c4<-table(tree.pred.good, actual.left.2)
#Error
tree.fit.err.good<-mean(tree.pred.good !=actual.left.2)
# tree.fit.err.good

######################Bagging Tree

set.seed(1)
bag.tree.fit.good<-randomForest(as.factor(left)~.,data=training.2, mtry=9,importance=TRUE)
#look at the ERROR
bag.tree.pred.good<-predict(bag.tree.fit.good,test.2, type="class")
c5<-table(bag.tree.pred.good, actual.left.2)
#Error
bag.tree.fit.err.good<-mean(bag.tree.pred.good !=actual.left.2)
# bag.tree.fit.err.good

######################BOOSTING

#mutate left training.2 to 0 and 1
#training.2<-mutate(training.2, left.Binary=(ifelse(left=="Yes", 1, 0)))
#run boost
boost.tree.fit.good<-gbm(left~., data=training.2, distribution="bernoulli", n.trees = 500, interaction.depth=2)
# summary(boost.tree.fit.good)

#look at the ERROR
boost.tree.pred.good<-round(predict(boost.tree.fit.good,test.2, type="response", n.trees=500))
c6<-table(boost.tree.pred.good, actual.left.2)
#Error
boost.tree.fit.err.good<-1-((2260)/(740+2260))
# boost.tree.fit.err.good

######################################NEURAL NETWORKS

#SINGLE LAYER...
#single layer
ns <- nnet(left ~ .,data=training.2,size=5,linout=FALSE,decay=5e-4)

#Predict
pr.ns.good <- predict(ns,test.2[,1:9])
pr.nn_2.good <- pr.ns.good*sd(data.2$left) + mean(data.2$left)
mse.ns.good <- sum((test.2$left - pr.nn_2.good)^2)/nrow(pr.nn_2.good)
# mse.ns.good

##############################
models.good<-c("LOG REG","LDA","QDA","CLASS. TREE", "BAGGING",
          "BOOSTING","NEURAL NETWORKS")

ter.result.good<-c(glm.fit.err.good,lda.fit.err.good,qda.fit.err.good,tree.fit.err.good,bag.tree.fit.err.good,boost.tree.fit.err.good,mse.ns.good)
ter.table.good<-data.frame(Classifier= models.good,TER_GOOD = ter.result.good) %>% arrange(TER_GOOD)

```
## Machine Learning: Test Error Rate Summary
- I can now compare the TER for both datasets and choose a classifier with the best performance.
```{r, echo=FALSE}

finalcom<-ter.table.good %>% inner_join(ter.table, by = "Classifier") %>% arrange(TER_GOOD)

 plot_ly(finalcom, x = ~Classifier, y = ~TER_GOOD, type = 'bar', name = 'TER_GOOD') %>%
  add_trace(y = ~TER_ALL, name = 'TER_ALL') %>%
  layout(title = "Comparison of TER from 'Good' and 'All' datasets",
    yaxis = list(title = 'TER'), barmode = 'group')
```

## Machine Learning: Results

* Now that we know that Bagging Method performs the best, we can see what makes most experienced and best employees leave.

* The following variables contribute the most to best and most experienced employees to leave the company:
    + Satisfaction Level
    + Time Spent at Company
    + Number of Projects


```{r, echo=TRUE}
importance(bag.tree.fit.good)
```

## Whats Next?

* Manipulate each classifier with different parameters:
    + Use CV where appropriate to obtain best performance.
* Explore different classifiers:
    + EX: Multi-Layer Neural Networks (library: neuralnet)
* Explore different visualization techniques.
* Use Python.
* Use more of the HTML and CSS to make presentation more aesthetically pleasing.
