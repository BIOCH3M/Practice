<style>
body {overflow:scroll;} </style>

---
title: "HR_Presentation"
author: "Alexandr Surin"
date: "August 29, 2017"
output: ioslides_presentation
---
## General:



## Linear Approach

* Linear Model
* Y = a + BX + e 
    + Y is a dummy dependent variable, =1 if event happens, =0 if event doesn't happen, 
    + a is the coefficient on the constant term, 
    + B is the coefficient(s) on the independent variable(s), 
    + X is the independent variable(s), and 
    + e is the error term. 
* Linear model has problems:
    + Errors terms are Heteroskedastic (when variability of a variable Y changes with changes in X)
    + e is not normally distributed (classical regression assumption fails)
    + outside of 1 and 0


## Logistic Approach

* [p/(1-p)] = exp(a + BX + e)
    + ln is the natural logarithm, logexp, where exp=2.71828. 
    + p is the probability that the event Y occurs, p(Y=1) 
    + p/(1-p) is the "odds ratio" 
    ln[p/(1-p)] is the log odds ratio, or "logit" 
    + all other components of the model are the same.
    
*Basically it's just a nonlinear transformation of Linear model

*For instance, the estimated probability is: 
    + p = 1/[1 + exp(-a - BX)] 

* With this functional form: 
    + if you let a + BX =0, then p = .50 
    + as a + BX gets really big, p approaches 1 
    + as a + BX gets really small, p approaches 0. 

* B Coefficient is just the RATE OF CHANGE IN Y

## LDA AND QDA: Discriminant Analysis:

* Discriminant Analysis:Advantage
    + More Stable than LDA
    + More than two non-ordinal response classes (categorical variables)
    + Assumptions:
    + X are normal
    + equal covariance among predictors
* LDA
    + Computes discriminant scores for each observation to classify respnose variable:
    + Computes probability distribution of being classified as class A or B
* QDA
    + Assumes normal distribution of X but doesn't have the assumption that there is common variance among predictors
    
## Classification Tree:

* Devision of universe into subproblems until (creating nodes) until one class shows majority devision (leaf)
    + Start at root Node and keep going 
    + Gini Index: measure of inequality...measure between 0 (Same category) and 1 (different category).
    
## Bagging (Bootstrap Aggregating)

1. Generates  several training sets using random sampling with replacement
2. Applies classification tree algorithm to each set
3. Takes majority vote to classify data

## Boosting

1. Builds successively traning models based on misclassified records in previous models. 
2. All classifiers are combined by weighted majority vote

## Random Tree (Bagging)

1. Traing multiple weak class trees using fixed number of randomly selected features for classification, then takes modeof each class to create stronger classifier.


## Neural Networks

* Has three layers:
  + Input
  + Hidden Layers (Single in nnet)
  + Output Layer
* Each layer has processing units which have thier own inputs.
  

    