<style>
body {
  overflow: scroll;
}
</style>



---
title: "Text Mining Practice"
author: "Alexandr Surin"
date: "October 20, 2017"
output: ioslides_presentation
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)

library(gutenbergr)#Book
library(janeaustenr)#Book
library(tidyverse)
library(ggraph)
library(ggplot2)
library(tidytext)
library(stringr)
library(scales)
library(wordcloud)
library(reshape2)
library(igraph)#networks
library(widyr)#counts and correlations
library(tm)#Document Term Matrix
library(topicmodels) #Topic Modeling

```


## Text Mining In R

* Purpose:
    + To Practice Text mining using [A Tidy Approach](http://tidytextmining.com/tidytext.html#contrasting-tidy-text-with-other-data-structures)

## Exploration Goals:

- Tidy Text Data Exploration
- Sentiment Analysis
- Analysing Word and Document Frequency
- Relationships Between Words
- Converting to and from Tidy Format 
- Topic modeling

## Vocabulary:
    + **String**: Text can, of course, be stored as strings, i.e., character vectors, within R, and often text data is first read into memory in this form. 
    + **Corpus**: These types of objects typically contain raw strings annotated with additional metadata and details.
    + ** Document-Term Matrix**:This is a sparse matrix describing a collection (i.e., a corpus) of documents with one row for each document and one column for each term. The value in the matrix is typically word count

# TIDY TEXT FORMAT

## Small Poem

```{r,echo = TRUE}
text <- c("Because I could not stop for Death -",
          "He kindly stopped for me -",
          "The Carriage held but just Ourselves -",
          "and Immortality")
text
#start converting into tidy format!
text_df <- data_frame(line = 1:4, text = text)
text_df


text_df %>%
  unnest_tokens(word, text) #first argument is new column name, second is the name of the column in text_df!
#line column represents which line the word came from!
#By default unnest_tokens lower cases, and punctuations get stripped
```

## Jane Austen: Tidying 

```{r,echo = TRUE}


original_books <- austen_books() %>%
  group_by(book) %>%
  mutate(linenumber = row_number(),
         chapter = cumsum(str_detect(text, regex("^chapter [\\divxlc]",#starts with chapter and has numbers
                                                 ignore_case = TRUE)))) %>%
  ungroup()

original_books

```

## Jane Austen: 'one-token-per-row'
    - Seperates each line of text into original data frame into tokens. (Default tokenization is words but can be tokenized by characters, n-grams, sentences, etc.)
```{r,echo = TRUE}
tidy_books <- original_books %>%
  unnest_tokens(word, text)

tidy_books
```

## Jane Austen: Removing stop words

* Dataset of all 'stop words' can be found in a package...so we will just use anti-join to remove all of them.
* This is a good starting point for any visualizations which you find fit for analysis.

```{r,echo = TRUE}    
tidy_books <- tidy_books %>%
  anti_join(stop_words)
#Count Words!
tidy_books %>%
  count(word, sort = TRUE)  
#Now you can make visualizations if desired.
    
```   


## Gutenberg Collection: Word Frequencies

```{r,echo = TRUE} 
#Download only selected books by HG Wells
hgwells <- gutenberg_download(c(35, 36, 5230, 159))
# Repeat one-token-per-row and remove all stop words
tidy_hgwells <- hgwells %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_words)
#Count workds
tidy_hgwells %>%
  count(word, sort = TRUE)

#Download only selected books by Bronte

bronte <- gutenberg_download(c(1260, 768, 969, 9182, 767))

tidy_bronte <- bronte %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_words)
tidy_bronte %>%
  count(word, sort = TRUE)
```


```{r,echo = TRUE} 
#Now combine all this information into a single dataframe
frequency <- bind_rows(mutate(tidy_bronte, author = "Brontë Sisters"),
                       mutate(tidy_hgwells, author = "H.G. Wells"), 
                       mutate(tidy_books, author = "Jane Austen")) %>% 
  mutate(word = str_extract(word, "[a-z']+")) %>%#this strips the words of anything that is not text
  count(author, word) %>%
  group_by(author) %>%
  mutate(proportion = n / sum(n)) %>% 
  select(-n) %>% 
  spread(author, proportion) %>% 
  gather(author, proportion, `Brontë Sisters`:`H.G. Wells`)



```

## Comparing WF of Jane, Bronte and Wells

* Words that are close to the line in these plots have similar frequencies in both sets of texts.
```{r,echo = TRUE} 
ggplot(frequency, aes(x = proportion, y = `Jane Austen`, color = abs(`Jane Austen` - proportion))) +
  geom_abline(color = "gray40", lty = 2) +
  geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
  geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
  scale_x_log10(labels = percent_format()) +
  scale_y_log10(labels = percent_format()) +
  scale_color_gradient(limits = c(0, 0.001), low = "darkslategray4", high = "gray75") +
  facet_wrap(~author, ncol = 2) +
  theme(legend.position="none") +
  labs(y = "Jane Austen", x = NULL)
#missing values are ok here


```   
   
   
## Similarity of WF

* How correlated these different sets of word frequencies 

* Bronte vs Jane

```{r, echo=TRUE}

cor.test(data = frequency[frequency$author == "Brontë Sisters",],
         ~ proportion + `Jane Austen`)


```

* Wells vs Jane

```{r, echo=TRUE}

cor.test(data = frequency[frequency$author == "H.G. Wells",], 
         ~ proportion + `Jane Austen`)
         
```

* From the Correlation test we see that word frequencies are much more correlated between Jane and Bronte than Jane and Wells


# SENTIMENT ANALYSIS

## Sentiments Dataset Aproach (Unigram)

* Use Sentiment of individual words and sentiment content of the whole text as the sum of the sentiment content

* Tidytext package has three dictionaries:
    + AFINN:
        - Lexicon assigns words wwith a score that runs beween -5(negative) and 5(positive)
    + bing:
        - Lexicon categorizes words as binary variable (positive and negative)
    + nrc:
        - Lexicon categorizes words in into categories of positive, negative, anger, anticipation, disgust, fear, joy, sadness, surprise, and trust as binary variable
        
* 'bing' dictionary example
```{r, echo=TRUE}        
get_sentiments("bing")
```


## Obtaining Sentiment: Inner Join

```{r, echo=TRUE}
tidy_books <- austen_books() %>%
  group_by(book) %>%
  mutate(linenumber = row_number(),
         chapter = cumsum(str_detect(text, regex("^chapter [\\divxlc]", 
                                                 ignore_case = TRUE)))) %>%
  ungroup() %>%
  unnest_tokens(word, text)#create column word to just join over it with dictionary

nrcjoy <- get_sentiments("nrc") %>% 
  filter(sentiment == "joy") #one of the categories from nrc dict.

tidy_books %>%
  filter(book == "Emma") %>%
  inner_join(nrcjoy) %>%
  count(word, sort = TRUE)

```

## Obtaining Sentiment: By Sections

* We want to obtain sentiment based on sections of 100 lines of text.
* Net Sentiment = positive-negative
* We can also observe how sentiment changes over trajectory of each story
```{r, echo=TRUE}

# the %/% operator is interger division

janeaustensentiment <- tidy_books %>%
  inner_join(get_sentiments("bing")) %>%
  count(book, index = linenumber %/% 80, sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative)
###PLOT 
ggplot(janeaustensentiment, aes(index, sentiment, fill = book)) +#aestetic mapping
  geom_col(show.legend = FALSE) +
  facet_wrap(~book, ncol = 2, scales = "free_x")

```

## Obtaining Sentiment: Most Positive and Negative Words

```{r, echo=TRUE}

bing_word_counts <- tidy_books %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup()

bing_word_counts

####Now plot it...
bing_word_counts %>%
  group_by(sentiment) %>%
  top_n(10) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(y = "Contribution to sentiment",
       x = NULL) +
  coord_flip()
```

* Notice the anomaly: 'miss' is not negative it's female... so now we need to add this word to our 'stop word' dictionary

```{r, echo=TRUE}
custom_stop_words <- bind_rows(data_frame(word = c("miss"), 
                                          lexicon = c("custom")), 
                               stop_words)

custom_stop_words

```

## Obtaining Sentiment: Worldclouds

```{r, echo=TRUE}

tidy_books %>%
  anti_join(stop_words) %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 30))

```

## Obtaining Sentiment: Comparing Worldclouds

```{r, echo=TRUE}

tidy_books %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%#this function turns data frame into matrix using reshape2
  comparison.cloud(colors = c("#F8766D", "#00BFC4"),
                   max.words = 30)

```


## Obtaining Sentiment: Going beyond Single Words(Unigrams)

* We can try to run NLP to tokenize text into sentences via packages like: coreNLP, cleanNLP and sentimentr.

```{r, echo=TRUE}

PandP_sentences <- data_frame(text = prideprejudice) %>% 
  unnest_tokens(sentence, text, token = "sentences")

PandP_sentences$sentence[2]
#we can also tokenize by chapter

austen_chapters <- austen_books() %>%
  group_by(book) %>%
  unnest_tokens(chapter, text, token = "regex", 
                pattern = "Chapter|CHAPTER [\\dIVXLC]") %>%
  ungroup()

austen_chapters %>% 
  group_by(book) %>% 
  summarise(chapters = n())

```

## Sentiment: Which chapter has the most negative sentiment

```{r, echo=FALSE}
bingnegative <- get_sentiments("bing") %>% 
  filter(sentiment == "negative")

wordcounts <- tidy_books %>%
  group_by(book, chapter) %>%
  summarize(words = n())

tidy_books %>%
  semi_join(bingnegative) %>%
  group_by(book, chapter) %>%
  summarize(negativewords = n()) %>%
  left_join(wordcounts, by = c("book", "chapter")) %>%
  mutate(ratio = negativewords/words) %>%
  filter(chapter != 0) %>%
  top_n(1) %>%
  ungroup()

```

# Analyzing Word and Document Frequency: tf-idf

## Intro:

1.  **Term Frequency**: Measure of how important a word may be.
 
2. **Inverse Document Frequency**: Decreases the weight for commonly used words and increases the weight for words that are not used much.

3. **tf-idf**: is the combination of both measurements and is intended to measure how important a word is to a document in a corpus of ducuments. 

4. idf=ln(ndocuments/ndocuments containing term)

```{r, echo=TRUE}

book_words <- austen_books() %>%
  unnest_tokens(word, text) %>%
  count(book, word, sort = TRUE) %>%
  ungroup()

total_words <- book_words %>% 
  group_by(book) %>% 
  summarize(total = sum(n))

book_words <- left_join(book_words, total_words)

book_words


#Now plot

#The long tails are the really common words!
ggplot(book_words, aes(n/total, fill = book)) +
  geom_histogram(show.legend = FALSE) +
  xlim(NA, 0.0009) +
  facet_wrap(~book, ncol = 2, scales = "free_y")

```

## Zipf's Law

* Frequencythat a word appears is inversely proportional to it's rank

* The rank column will tell the rank of each word within the freqency table


```{r, echo=TRUE}
#Find Freq by Rank
freq_by_rank <- book_words %>% 
  group_by(book) %>% 
  mutate(rank = row_number(), 
         `term frequency` = n/total)

freq_by_rank


```

* Zipf's Law is visualized by plotting rank on x-axis and term freqency on the y on log scales.
```{r, echo=TRUE}
freq_by_rank %>% 
  ggplot(aes(rank, `term frequency`, color = book)) + 
  geom_line(size = 1.2, alpha = 0.8) + 
  scale_x_log10() +
  scale_y_log10()
```

* We see that everythign is in log-lg scale and that all 6 novels are similar to each other and that the relationship between rank and frequency does have a negative slope. 

Now we can examine the curve using **power law**

* [Power Law](https://en.wikipedia.org/wiki/Power_law)
* Looking at the middle section only!

```{r, echo=TRUE}
rank_subset <- freq_by_rank %>% 
  filter(rank < 500,
         rank > 10)

lm(log10(`term frequency`) ~ log10(rank), data = rank_subset)


```

* Here we are looking at the slope being close to -1.
```{r, echo=TRUE}

freq_by_rank %>% 
  ggplot(aes(rank, `term frequency`, color = book)) + 
  geom_abline(intercept = -0.62, slope = -1.1, color = "gray50", linetype = 2) +
  geom_line(size = 1.2, alpha = 0.8) + 
  scale_x_log10() +
  scale_y_log10()
```



## Find Important Words in Document

* We use the tf-idf to determine important words by decreasing weight of common words and increase weight of words which aren't used much.

* bind_tf_idf function
    + All common words will have tf-idf at zero.
    + The idf term (natural log of 1) is zero because they appear in all books.
    + the tf-idf(inverse document frequency) is almost zero, giving the tokens low weight
```{r, echo=TRUE}

book_words <- book_words %>%
  bind_tf_idf(word, book, n)
book_words
```
* Now we can look at terms that are in fact important based on the high tf-idf.
* Some values are the smae for different terms because there are 6 douments in this corpus and we are seeing the numerical values for ln(6/1), ln(6/2), etc.

``` {r, echo = TRUE}
book_words %>%
  select(-total) %>%
  arrange(desc(tf_idf))
```

* Visualization for high-tf words

``` {r, echo = TRUE}

plot_austen <- book_words %>%
  arrange(desc(tf_idf)) %>%
  mutate(word = factor(word, levels = rev(unique(word))))

plot_austen %>% 
  top_n(20) %>%
  ggplot(aes(word, tf_idf, fill = book)) +
  geom_col() +
  labs(x = NULL, y = "tf-idf") +
  coord_flip()

```
* And now by Novel:

```{r, echo=TRUE}

plot_austen %>% 
  group_by(book) %>% 
  top_n(15) %>% 
  ungroup %>%
  ggplot(aes(word, tf_idf, fill = book)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "tf-idf") +
  facet_wrap(~book, ncol = 2, scales = "free") +
  coord_flip()

```

# Relationships Between Words: n-grams and correlations

## Intro

* Which words seem to co-occur with in the same docuemnts
* Argument *token='ngrams'* tokenizes by pairs adjacent words where n is the number of words captured i.e. n=2 is called bigrams.
* Package "ggprah"(network plots) and "widyr"(calculates pairwise correlations and distances)

```{r, echo=FALSE}
library(ggraph)
library(widyr)
```

* See how often word X is followed by word Y and build a model of relationships between them.
```{r, echo=TRUE}

austen_bigrams <- austen_books() %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2)

austen_bigrams

```

## Counting and Filtering n-grams
```{r, echo=TRUE}
austen_bigrams %>%
  count(bigram, sort = TRUE)

```
* This includes a lot of stop words... so let's remove all stop words!

```{r, echo=TRUE}

bigrams_separated <- austen_bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ")

bigrams_filtered <- bigrams_separated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)
# new bigram counts:
bigram_counts <- bigrams_filtered %>% 
  count(word1, word2, sort = TRUE)

bigram_counts
```

## Analyzing Bigrams

* Looking for most common 'streets'

```{r, echo=TRUE}
bigrams_filtered %>%
  filter(word2 == "street") %>%
  count(book, word1, sort = TRUE)

```

* Since each bigram can be treated as individual words we can do tf-idf analysis
```{r, echo=TRUE}

bigrams_united <- bigrams_filtered %>%
  unite(bigram, word1, word2, sep = " ")

bigrams_united
bigram_tf_idf <- bigrams_united %>%
  count(book, bigram) %>%
  bind_tf_idf(bigram, book, n) %>%
  arrange(desc(tf_idf))

bigram_tf_idf

```

## Bigrams: Sentiment Analysis

* Doing sentiment analysis as bigrams opens new insight beacause it accounts for 'not happy' type phrases
* Start by summuary of words that precede by 'not'

```{r, echo=TRUE}

bigrams_separated %>%
  filter(word1 == "not") %>%
  count(word1, word2, sort = TRUE)
```

* Now find out which words are preceded by not:

```{r, echo=TRUE}
#sentiment dictionary
AFINN <- get_sentiments("afinn")
AFINN


#Look for words precedded by 'not'
not_words <- bigrams_separated %>%
  filter(word1 == "not") %>%
  inner_join(AFINN, by = c(word2 = "word")) %>%
  count(word2, score, sort = TRUE) %>%
  ungroup()
not_words

```
* Look for which words contribute to the wrong direction due unaccounting for missing
```{r, echo=TRUE}
not_words %>%
  mutate(contribution = n * score) %>%
  arrange(desc(abs(contribution))) %>%
  head(20) %>%
  mutate(word2 = reorder(word2, contribution)) %>%
  ggplot(aes(word2, n * score, fill = n * score > 0)) +
  geom_col(show.legend = FALSE) +
  xlab("Words preceded by \"not\"") +
  ylab("Sentiment score * number of occurrences") +
  coord_flip()
```
* Here the impact of the phrases of 'not like' are huge misidentifications making the text more positive

* Now we can pick a string of words that negate subsequent term and see how all of them are contributign

```{r, echo=TRUE}

negation_words <- c("not", "no", "never", "without")

negated_words <- bigrams_separated %>%
  filter(word1 %in% negation_words) %>%
  inner_join(AFINN, by = c(word2 = "word")) %>%
  count(word1, word2, score, sort = TRUE) %>%
  ungroup()
negated_words

```

## Bigrams: Visualization of Network

* Good way to visualize relationships amount words simultaneously by showing an entire network.
* Structure:
    + From: node an edge is coming from
    + To: node an edge is going to
    + Weight: Numeric value associated with each edge
    
* Function: graph_from_data_frame()
    + Takes data frame of edges with columns 'From' and 'To' and edge attributes which is n.
```{r, echo=TRUE}
# filter for only relatively common combinations
bigram_graph <- bigram_counts %>%
  filter(n > 20) %>%
  graph_from_data_frame()

bigram_graph
```
* Now graph in ggraph.

```{r, echo=TRUE}
set.seed(1)

ggraph(bigram_graph, layout = "fr") +
  geom_edge_link() +
  geom_node_point() +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1)

```


## Bigrams: Counting and Correlating Word pairs

* Use pairwise comparisons between groups of observations.
* For example you might want to find out in what words tend to appear within the same section

```{r, echo=TRUE}
austen_section_words <- austen_books() %>%
  filter(book == "Pride & Prejudice") %>%
  mutate(section = row_number() %/% 10) %>%
  filter(section > 0) %>%
  unnest_tokens(word, text) %>%
  filter(!word %in% stop_words$word)

austen_section_words
```
* Or we can have the pairwise count as well that will count pairs of words co-appearing within the same section

```{r, echo=TRUE}

# count words co-occuring within sections
word_pairs <- austen_section_words %>%
  pairwise_count(word, section, sort = TRUE)

word_pairs
```

## Correlations

* We can see how often words appear together relative to how often they appear separately.
* Phi Coefficient: Common measure of binary correlation
    + How much more likely it is that either *both* word X and Y appear or *neither* do, than that one appears without the other.
    + Same as the Person correlation (binary data)
    + Use funciton: pairwise_cor() to find phi coefficient
    
```{r, echo=TRUE}

# we need to filter for at least relatively common words first
word_cors <- austen_section_words %>%
  group_by(word) %>%
  filter(n() >= 20) %>%
  pairwise_cor(word, section, sort = TRUE)

word_cors

```
* To look for words associated with other words of interest

```{r, echo=TRUE}

word_cors %>%
  filter(item1 %in% c("elizabeth", "pounds", "married", "pride")) %>%
  group_by(item1) %>%
  top_n(6) %>%
  ungroup() %>%
  mutate(item2 = reorder(item2, correlation)) %>%
  ggplot(aes(item2, correlation)) +
  geom_bar(stat = "identity") +
  facet_wrap(~ item1, scales = "free") +
  coord_flip()

```

* Use ggraph to visualize correlations between bigrams


```{r, echo=TRUE}
word_cors %>%
  filter(correlation > .15) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) +
  geom_node_point(color = "lightblue", size = 5) +
  geom_node_text(aes(label = name), repel = TRUE) +
  theme_void()
```

# NON-TIDY TEXT

## DOCUMENT TERM MATRIX (DTM)

* DTM: Structure:
    + Each Row is a Document
    + Each Column is a Term
    + Each Value is a number of appearances of that term in a ducument
* To convert between tidy and non use 
    + tidy(): turns DTM into tidy data frame.
    + cast(): turns tidy into matrix
        - cast_sparse(): sparse matrix
        - cast_dtm(): DTM
        - cast_dfm(): converts DTM into quanteda
        
## DTM Continued (tm package)

* Get associated press docuemnt as data
* Look at Sparsity... pairs are zero.
* 
```{r, echo=TRUE}
#pull data
data("AssociatedPress", package = "topicmodels")
AssociatedPress
#distinct terms
terms <- Terms(AssociatedPress)
head(terms)
#Here only non-zero values are given
ap_td <- tidy(AssociatedPress)
ap_td
```

# Topic Modeling

## Topic Modeling: Intro
* Package *topicmodeling*:
    + Requires the data input in the form of DTM but produces models that can be tidied.

## Latnet Dirichlet allocation

* We assume that every document is a mixture of topics
* Every topic is a mixture of words
* *LDA*:
    + Finds mixture of words associated with each topic
    + Determines mixture of topics that describes each document
    
```{r, echo=TRUE}

data("AssociatedPress")
AssociatedPress

#Now use k=2 as a parameter of LDA() to create  a model with two topics

ap_lda <- LDA(AssociatedPress, k = 2, control = list(seed = 1234))
ap_lda

```
## Topic Modeling: Word-Topic Probability

* *tidytext* method can extract per-topic-per-word probabilities known as B (*Beta*)
* Model:
    + one-topic-per-term-per-row format
    + For each combination the model computes probability of the term being generated from that topic
```{r, echo=TRUE}
#This extracts per topic per word probabilities
ap_topics <- tidy(ap_lda, matrix = "beta")
ap_topics

```

* Generate top 10 most common terms per topic.
```{r, echo=TRUE}

ap_top_terms <- ap_topics %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

ap_top_terms %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip()
```
* Or find terms that have greatest difference in *B* between the two topics
    + Determined by: *log*2(*B2*/*B1*)-----log makes difference symmetrical.
        - *B2* being twice as large leads to a log ratio of 1
        - *B1* being twice as large leads to a log ratio of -1
    + Constrain it to set of relevant word, filter for relatively common words where *B* is greater than 1/1000 in at least one topic
    
```{r, echo=TRUE}
beta_spread <- ap_topics %>%
  mutate(topic = paste0("topic", topic)) %>%
  spread(topic, beta) %>%
  filter(topic1 > .001 | topic2 > .001) %>%
  mutate(log_ratio = log2(topic2 / topic1))
beta_spread

```

* Now we can visualize words with greatest difference in B between topics

## Topic Modeling: Document-Topic Probabilities

* Structure:
    + per-document-per-topic probabilities *gamma*
    + Values are an estimated proportion of words from that document that are generated from that topic
    
```{r, echo= TRUE}
ap_documents <- tidy(ap_lda, matrix = "gamma")
ap_documents

```
* To check the most common words in the document
```{r, echo= TRUE}

tidy(AssociatedPress) %>%
  filter(document == 6) %>%
  arrange(desc(count))

```

## Topic Modeling: Grouping Text Example (Library Heist)

* You have **4** books that need to be put together by individual chapters
* We can see how chapters cluster together into distinct topics each of them representing one of the books.

```{r, echo=TRUE}
titles <- c("Twenty Thousand Leagues under the Sea", "The War of the Worlds",
            "Pride and Prejudice", "Great Expectations")

books <- gutenberg_works(title %in% titles) %>%
  gutenberg_download(meta_fields = "title")

# separate chapters into words and remove stop words

by_chapter <- books %>%
  group_by(title) %>%
  mutate(chapter = cumsum(str_detect(text, regex("^chapter ", ignore_case = TRUE)))) %>%
  ungroup() %>%
  filter(chapter > 0) %>%
  unite(document, title, chapter)

# split into words
by_chapter_word <- by_chapter %>%
  unnest_tokens(word, text)

# find document-word counts
word_counts <- by_chapter_word %>%
  anti_join(stop_words) %>%
  count(document, word, sort = TRUE) %>%
  ungroup()

word_counts
```
## Topic Modeling: LDA on chapters
* First push data into DTM format
* Then use LDA function to creat4 four-topic model. 
```{r, echo=TRUE}
chapters_dtm <- word_counts %>%
  cast_dtm(document, word, n)

chapters_dtm

#Since we have four books.. we should have set k parameter to 4
chapters_lda <- LDA(chapters_dtm, k = 4, control = list(seed = 1234))
chapters_lda


```

* Now examine per-topic-per-word probabilities.


```{r, echo=TRUE}
#For each combination the model computes the probability of that term being generated from that topic
chapter_topics <- tidy(chapters_lda, matrix = "beta")
chapter_topics#Joe in this case has zero probability of being in book 1-3 but actually 1.45 percent probability from being in book four!

```
* Now find top 5 terms within each topic
* Visualization
```{r, echo=TRUE}

top_terms <- chapter_topics %>%
  group_by(topic) %>%
  top_n(5, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

top_terms
#Visualization
top_terms %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip()
```

## Topic Modeling: PER-DOCUMENT CLASSIFICATION

* If we want to find out which topics are associated with each document.
    + Use *gamma*  to examine per-document-per-topic probabilitities.
    
```{r, echo=TRUE}
# Find ESTIMATED proportion of words from that document generated from topic
chapters_gamma <- tidy(chapters_lda, matrix = "gamma")
chapters_gamma

# Now separate the document name into title and chapter and then visualize the per-document-per-topic probability for each
chapters_gamma <- chapters_gamma %>%
  separate(document, c("title", "chapter"), sep = "_", convert = TRUE)

chapters_gamma

# reorder titles in order of topic 1, topic 2, etc before plotting
chapters_gamma %>%
  mutate(title = reorder(title, gamma * topic)) %>%
  ggplot(aes(factor(topic), gamma)) +
  geom_boxplot() +
  facet_wrap(~ title)



```
* Looking at the graph above we see that only *Great Expectations* had some chapters that were associated with other topics.

* NOW LOOK AT MISCLASSIFICATION:
```{r, echo=TRUE}
chapter_classifications <- chapters_gamma %>%
  group_by(title, chapter) %>%
  top_n(1, gamma) %>%
  ungroup()

chapter_classifications

book_topics <- chapter_classifications %>%
  count(title, topic) %>%
  group_by(title) %>%
  top_n(1, n) %>%
  ungroup() %>%
  transmute(consensus = title, topic)

chapter_classifications %>%
  inner_join(book_topics, by = "topic") %>%
  filter(title != consensus)
``` 

## Topic Modeling: AUGMENT (BY WORD ASSIGNMENTS)

* The more words in a docuemnt assigned to a topic the more weight(gamma) will go on that docuemnt-topic classification.
* Function *augment()* adds information to each raw data observation ".topic"
```{r, echo=TRUE}

assignments <- augment(chapters_lda, data = chapters_dtm)
assignments
#look for titles that were misclassified 
#title=true book vs concensus(assigned)
assignments <- assignments %>%
  separate(document, c("title", "chapter"), sep = "_", convert = TRUE) %>%
  inner_join(book_topics, by = c(".topic" = "topic"))
assignments

```

* Create a confusion matrix showing how often owrds fron one book were assigned to another

```{r, echo=TRUE}
assignments %>%
  count(title, consensus, wt = count) %>%
  group_by(title) %>%
  mutate(percent = n / sum(n)) %>%
  ggplot(aes(consensus, title, fill = percent)) +
  geom_tile() +
  scale_fill_gradient2(high = "red", label = percent_format()) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1),
        panel.grid = element_blank()) +
  labs(x = "Book words were assigned to",
       y = "Book words came from",
       fill = "% of assignments")

```

* From the confusion matrix above:
    + 'Almost all the words for Pride and Prejudice, Twenty Thousand Leagues Under the Sea, and War of the Worlds were correctly assigned, while Great Expectations had a fair number of misassigned words (which, as we saw above, led to two chapters getting misclassified)'

* Now look at the wrong words only:
```{r, echo=TRUE}

wrong_words <- assignments %>%
  filter(title != consensus)
wrong_words


wrong_words %>%
  count(title, consensus, term, wt = count) %>%
  ungroup() %>%
  arrange(desc(n))

```

### WHAT I NEED:
* SQL Management Studio 2016
* R Studio
* R 






