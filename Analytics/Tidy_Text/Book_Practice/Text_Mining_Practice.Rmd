<style>
body {
  overflow: scroll;
}
</style>



---
title: "Text Mining Practice"
author: "Alexandr Surin"
date: "October 20, 2017"
output: ioslides_presentation
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)

library(gutenbergr)#Book
library(janeaustenr)#Book
library(tidyverse)
library(ggplot2)
library(tidytext)
library(stringr)
library(scales)
library(wordcloud)
library(reshape2)

```


## Text Mining In R

* Purpose:
    + To Practice Text mining using [A Tidy Approach](http://tidytextmining.com/tidytext.html#contrasting-tidy-text-with-other-data-structures)

## Exploration Goals:

- Tidy Text Data Exploration
- Sentiment Analysis
- Analysing Word and Document Frequency
- Relationships Between Words
- Converting to and from Tidy Format 
- Topic modeling

## Vocabulary:
    + **String**: Text can, of course, be stored as strings, i.e., character vectors, within R, and often text data is first read into memory in this form. 
    + **Corpus**: These types of objects typically contain raw strings annotated with additional metadata and details.
    + ** Document-Term Matrix**:This is a sparse matrix describing a collection (i.e., a corpus) of documents with one row for each document and one column for each term. The value in the matrix is typically word count

# TIDY TEXT FORMAT

## Small Poem

```{r,echo = TRUE}
text <- c("Because I could not stop for Death -",
          "He kindly stopped for me -",
          "The Carriage held but just Ourselves -",
          "and Immortality")
text
#start converting into tidy format!
text_df <- data_frame(line = 1:4, text = text)
text_df


text_df %>%
  unnest_tokens(word, text) #first argument is new column name, second is the name of the column in text_df!
#line column represents which line the word came from!
#By default unnest_tokens lower cases, and punctuations get stripped
```

## Jane Austen: Tidying 

```{r,echo = TRUE}


original_books <- austen_books() %>%
  group_by(book) %>%
  mutate(linenumber = row_number(),
         chapter = cumsum(str_detect(text, regex("^chapter [\\divxlc]",#starts with chapter and has numbers
                                                 ignore_case = TRUE)))) %>%
  ungroup()

original_books

```

## Jane Austen: 'one-token-per-row'
    - Seperates each line of text into original data frame into tokens. (Default tokenization is words but can be tokenized by characters, n-grams, sentences, etc.)
```{r,echo = TRUE}
tidy_books <- original_books %>%
  unnest_tokens(word, text)

tidy_books
```

## Jane Austen: Removing stop words

* Dataset of all 'stop words' can be found in a package...so we will just use anti-join to remove all of them.
* This is a good starting point for any visualizations which you find fit for analysis.

```{r,echo = TRUE}    
tidy_books <- tidy_books %>%
  anti_join(stop_words)
#Count Words!
tidy_books %>%
  count(word, sort = TRUE)  
#Now you can make visualizations if desired.
    
```   


## Gutenberg Collection: Word Frequencies

```{r,echo = TRUE} 
#Download only selected books by HG Wells
hgwells <- gutenberg_download(c(35, 36, 5230, 159))
# Repeat one-token-per-row and remove all stop words
tidy_hgwells <- hgwells %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_words)
#Count workds
tidy_hgwells %>%
  count(word, sort = TRUE)

#Download only selected books by Bronte

bronte <- gutenberg_download(c(1260, 768, 969, 9182, 767))

tidy_bronte <- bronte %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_words)
tidy_bronte %>%
  count(word, sort = TRUE)
```


```{r,echo = TRUE} 
#Now combine all this information into a single dataframe
frequency <- bind_rows(mutate(tidy_bronte, author = "Brontë Sisters"),
                       mutate(tidy_hgwells, author = "H.G. Wells"), 
                       mutate(tidy_books, author = "Jane Austen")) %>% 
  mutate(word = str_extract(word, "[a-z']+")) %>%#this strips the words of anything that is not text
  count(author, word) %>%
  group_by(author) %>%
  mutate(proportion = n / sum(n)) %>% 
  select(-n) %>% 
  spread(author, proportion) %>% 
  gather(author, proportion, `Brontë Sisters`:`H.G. Wells`)



```

## Comparing WF of Jane, Bronte and Wells

* Words that are close to the line in these plots have similar frequencies in both sets of texts.
```{r,echo = TRUE} 
ggplot(frequency, aes(x = proportion, y = `Jane Austen`, color = abs(`Jane Austen` - proportion))) +
  geom_abline(color = "gray40", lty = 2) +
  geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
  geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
  scale_x_log10(labels = percent_format()) +
  scale_y_log10(labels = percent_format()) +
  scale_color_gradient(limits = c(0, 0.001), low = "darkslategray4", high = "gray75") +
  facet_wrap(~author, ncol = 2) +
  theme(legend.position="none") +
  labs(y = "Jane Austen", x = NULL)
#missing values are ok here


```   
   
   
## Similarity of WF

* How correlated these different sets of word frequencies 

* Bronte vs Jane

```{r, echo=TRUE}

cor.test(data = frequency[frequency$author == "Brontë Sisters",],
         ~ proportion + `Jane Austen`)


```

* Wells vs Jane

```{r, echo=TRUE}

cor.test(data = frequency[frequency$author == "H.G. Wells",], 
         ~ proportion + `Jane Austen`)
         
```

* From the Correlation test we see that word frequencies are much more correlated between Jane and Bronte than Jane and Wells


# SENTIMENT ANALYSIS

## Sentiments Dataset Aproach (Unigram)

* Use Sentiment of individual words and sentiment content of the whole text as the sum of the sentiment content

* Tidytext package has three dictionaries:
    + AFINN:
        - Lexicon assigns words wwith a score that runs beween -5(negative) and 5(positive)
    + bing:
        - Lexicon categorizes words as binary variable (positive and negative)
    + nrc:
        - Lexicon categorizes words in into categories of positive, negative, anger, anticipation, disgust, fear, joy, sadness, surprise, and trust as binary variable
        
* 'bing' dictionary example
```{r, echo=TRUE}        
get_sentiments("bing")
```


## Obtaining Sentiment: Inner Join

```{r, echo=TRUE}
tidy_books <- austen_books() %>%
  group_by(book) %>%
  mutate(linenumber = row_number(),
         chapter = cumsum(str_detect(text, regex("^chapter [\\divxlc]", 
                                                 ignore_case = TRUE)))) %>%
  ungroup() %>%
  unnest_tokens(word, text)#create column word to just join over it with dictionary

nrcjoy <- get_sentiments("nrc") %>% 
  filter(sentiment == "joy") #one of the categories from nrc dict.

tidy_books %>%
  filter(book == "Emma") %>%
  inner_join(nrcjoy) %>%
  count(word, sort = TRUE)

```

## Obtaining Sentiment: By Sections

* We want to obtain sentiment based on sections of 100 lines of text.
* Net Sentiment = positive-negative
* We can also observe how sentiment changes over trajectory of each story
```{r, echo=TRUE}

# the %/% operator is interger division

janeaustensentiment <- tidy_books %>%
  inner_join(get_sentiments("bing")) %>%
  count(book, index = linenumber %/% 80, sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative)
###PLOT 
ggplot(janeaustensentiment, aes(index, sentiment, fill = book)) +#aestetic mapping
  geom_col(show.legend = FALSE) +
  facet_wrap(~book, ncol = 2, scales = "free_x")

```

## Obtaining Sentiment: Most Positive and Negative Words

```{r, echo=TRUE}

bing_word_counts <- tidy_books %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup()

bing_word_counts

####Now plot it...
bing_word_counts %>%
  group_by(sentiment) %>%
  top_n(10) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(y = "Contribution to sentiment",
       x = NULL) +
  coord_flip()
```

* Notice the anomaly: 'miss' is not negative it's female... so now we need to add this word to our 'stop word' dictionary

```{r, echo=TRUE}
custom_stop_words <- bind_rows(data_frame(word = c("miss"), 
                                          lexicon = c("custom")), 
                               stop_words)

custom_stop_words

```

## Obtaining Sentiment: Worldclouds

```{r, echo=TRUE}

tidy_books %>%
  anti_join(stop_words) %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 30))

```

## Obtaining Sentiment: Comparing Worldclouds

```{r, echo=TRUE}

tidy_books %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%#this function turns data frame into matrix using reshape2
  comparison.cloud(colors = c("#F8766D", "#00BFC4"),
                   max.words = 30)

```


## Obtaining Sentiment: Going beyond Single Words(Unigrams)

* We can try to run NLP to tokenize text into sentences via packages like: coreNLP, cleanNLP and sentimentr.

```{r, echo=TRUE}

PandP_sentences <- data_frame(text = prideprejudice) %>% 
  unnest_tokens(sentence, text, token = "sentences")

PandP_sentences$sentence[2]
#we can also tokenize by chapter

austen_chapters <- austen_books() %>%
  group_by(book) %>%
  unnest_tokens(chapter, text, token = "regex", 
                pattern = "Chapter|CHAPTER [\\dIVXLC]") %>%
  ungroup()

austen_chapters %>% 
  group_by(book) %>% 
  summarise(chapters = n())

```

## Sentiment: Which chapter has the most negative sentiment

```{r, echo=FALSE}
bingnegative <- get_sentiments("bing") %>% 
  filter(sentiment == "negative")

wordcounts <- tidy_books %>%
  group_by(book, chapter) %>%
  summarize(words = n())

tidy_books %>%
  semi_join(bingnegative) %>%
  group_by(book, chapter) %>%
  summarize(negativewords = n()) %>%
  left_join(wordcounts, by = c("book", "chapter")) %>%
  mutate(ratio = negativewords/words) %>%
  filter(chapter != 0) %>%
  top_n(1) %>%
  ungroup()

```

# Analyzing Word and Document Frequency: tf-idf

## Intro:

1.  **Term Frequency**: Measure of how important a word may be.
 
2. **Inverse Document Frequency**: Decreases the weight for commonly used words and increases the weight for words that are not used much.

3. **tf-idf**: is the combination of both measurements and is intended to measure how important a word is to a document in a corpus of ducuments. 

4. idf=ln(ndocuments/ndocuments containing term)

```{r, echo=TRUE}

book_words <- austen_books() %>%
  unnest_tokens(word, text) %>%
  count(book, word, sort = TRUE) %>%
  ungroup()

total_words <- book_words %>% 
  group_by(book) %>% 
  summarize(total = sum(n))

book_words <- left_join(book_words, total_words)

book_words


#Now plot

#The long tails are the really common words!
ggplot(book_words, aes(n/total, fill = book)) +
  geom_histogram(show.legend = FALSE) +
  xlim(NA, 0.0009) +
  facet_wrap(~book, ncol = 2, scales = "free_y")

```

## Zipf's Law

* Frequencythat a word appears is inversely proportional to it's rank

* The rank column will tell the rank of each word within the freqency table


```{r, echo=TRUE}
#Find Freq by Rank
freq_by_rank <- book_words %>% 
  group_by(book) %>% 
  mutate(rank = row_number(), 
         `term frequency` = n/total)

freq_by_rank


```

* Zipf's Law is visualized by plotting rank on x-axis and term freqency on the y on log scales.
```{r, echo=TRUE}
freq_by_rank %>% 
  ggplot(aes(rank, `term frequency`, color = book)) + 
  geom_line(size = 1.2, alpha = 0.8) + 
  scale_x_log10() +
  scale_y_log10()
```

* We see that everythign is in log-lg scale and that all 6 novels are similar to each other and that the relationship between rank and frequency does have a negative slope. 

Now we can examine the curve using **power law**

* [Power Law](https://en.wikipedia.org/wiki/Power_law)
* Looking at the middle section only!

```{r, echo=TRUE}
rank_subset <- freq_by_rank %>% 
  filter(rank < 500,
         rank > 10)

lm(log10(`term frequency`) ~ log10(rank), data = rank_subset)


```

* Here we are looking at the slope being close to -1.
```{r, echo=TRUE}

freq_by_rank %>% 
  ggplot(aes(rank, `term frequency`, color = book)) + 
  geom_abline(intercept = -0.62, slope = -1.1, color = "gray50", linetype = 2) +
  geom_line(size = 1.2, alpha = 0.8) + 
  scale_x_log10() +
  scale_y_log10()
```


## 


